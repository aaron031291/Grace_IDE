"""
Grace_IDE Memory Component
Manages IDE state, history, caching, and intelligent memory systems
"""

import os
import json
import pickle
import sqlite3
import asyncio
import aiofiles
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, Tuple, Set
from dataclasses import dataclass, field, asdict
from collections import defaultdict, OrderedDict, deque
from abc import ABC, abstractmethod
import hashlib
import lz4.frame
import shutil
import threading
import weakref
import mmap
import tempfile
from enum import Enum
import numpy as np
from concurrent.futures import ThreadPoolExecutor


class CacheLevel(Enum):
    """Cache storage levels"""
    MEMORY = "memory"
    DISK = "disk"
    PERSISTENT = "persistent"


@dataclass
class MemoryEntry:
    """Represents an entry in memory"""
    key: str
    value: Any
    timestamp: datetime
    access_count: int = 0
    last_accessed: datetime = field(default_factory=datetime.now)
    ttl: Optional[int] = None  # Time to live in seconds
    size: int = 0
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class HistoryEntry:
    """Represents a history entry"""
    id: str
    action: str
    timestamp: datetime
    data: Dict[str, Any]
    user: Optional[str] = None
    session_id: Optional[str] = None
    reversible: bool = False
    undo_data: Optional[Dict] = None


@dataclass
class SessionState:
    """Represents IDE session state"""
    session_id: str
    created_at: datetime
    last_active: datetime
    open_files: List[str] = field(default_factory=list)
    active_file: Optional[str] = None
    cursor_positions: Dict[str, Dict] = field(default_factory=dict)
    breakpoints: Dict[str, List[int]] = field(default_factory=dict)
    terminal_sessions: List[str] = field(default_factory=list)
    layout: Dict[str, Any] = field(default_factory=dict)
    preferences: Dict[str, Any] = field(default_factory=dict)
    
    
class MemoryManager:
    """Main memory management system for Grace_IDE"""
    
    def __init__(self, workspace_root: str, max_memory_mb: int = 512):
        self.workspace_root = Path(workspace_root)
        self.max_memory = max_memory_mb * 1024 * 1024  # Convert to bytes
        self.memory_dir = self.workspace_root / '.grace' / 'memory'
        self.memory_dir.mkdir(parents=True, exist_ok=True)
        
        # Different memory stores
        self.cache = CacheManager(self.memory_dir / 'cache')
        self.history = HistoryManager(self.memory_dir / 'history')
        self.state = StateManager(self.memory_dir / 'state')
        self.index = IndexManager(self.memory_dir / 'index')
        self.semantic = SemanticMemory(self.memory_dir / 'semantic')
        
        # Session management
        self.sessions: Dict[str, SessionState] = {}
        self.active_session: Optional[str] = None
        
        # Memory pressure monitoring
        self.memory_monitor = MemoryMonitor(self.max_memory)
        self._monitor_task = None
        
        # Background tasks
        self._cleanup_task = None
        self._persist_task = None
        
    async def start(self):
        """Start memory manager"""
        await self.cache.start()
        await self.history.start()
        await self.state.start()
        await self.index.start()
        await self.semantic.start()
        
        # Start background tasks
        self._monitor_task = asyncio.create_task(self.memory_monitor.monitor())
        self._cleanup_task = asyncio.create_task(self._cleanup_loop())
        self._persist_task = asyncio.create_task(self._persist_loop())
        
        print("Memory manager started")
        
    async def stop(self):
        """Stop memory manager"""
        # Save all state
        await self.save_all()
        
        # Stop components
        await self.cache.stop()
        await self.history.stop()
        await self.state.stop()
        await self.index.stop()
        await self.semantic.stop()
        
        # Cancel background tasks
        for task in [self._monitor_task, self._cleanup_task, self._persist_task]:
            if task:
                task.cancel()
                
        print("Memory manager stopped")
        
    async def remember(self, key: str, value: Any, 
                      ttl: Optional[int] = None,
                      metadata: Optional[Dict] = None) -> bool:
        """Store a value in memory"""
        # Check memory pressure
        size = self._estimate_size(value)
        if not await self.memory_monitor.can_allocate(size):
            # Try to free memory
            await self._free_memory(size)
            
        # Store in appropriate location
        if size < 1024 * 1024:  # Less than 1MB - keep in memory
            return await self.cache.set(key, value, ttl, CacheLevel.MEMORY, metadata)
        else:  # Larger items go to disk
            return await self.cache.set(key, value, ttl, CacheLevel.DISK, metadata)
            
    async def recall(self, key: str) -> Optional[Any]:
        """Retrieve a value from memory"""
        return await self.cache.get(key)
        
    async def forget(self, key: str) -> bool:
        """Remove a value from memory"""
        return await self.cache.delete(key)
        
    async def search(self, pattern: str, limit: int = 10) -> List[Tuple[str, Any]]:
        """Search memory by pattern"""
        results = []
        
        # Search cache
        cache_results = await self.cache.search(pattern, limit)
        results.extend(cache_results)
        
        # Search semantic memory
        semantic_results = await self.semantic.search(pattern, limit)
        results.extend(semantic_results)
        
        # Sort by relevance and limit
        results.sort(key=lambda x: x[1].access_count if hasattr(x[1], 'access_count') else 0, reverse=True)
        return results[:limit]
        
    async def create_session(self, session_id: Optional[str] = None) -> str:
        """Create a new session"""
        if not session_id:
            session_id = hashlib.sha256(
                f"{datetime.now().isoformat()}{os.getpid()}".encode()
            ).hexdigest()[:16]
            
        session = SessionState(
            session_id=session_id,
            created_at=datetime.now(),
            last_active=datetime.now()
        )
        
        self.sessions[session_id] = session
        self.active_session = session_id
        
        # Persist session
        await self.state.save_session(session)
        
        return session_id
        
    async def restore_session(self, session_id: str) -> Optional[SessionState]:
        """Restore a session"""
        # Try memory first
        if session_id in self.sessions:
            return self.sessions[session_id]
            
        # Try persistent storage
        session = await self.state.load_session(session_id)
        if session:
            self.sessions[session_id] = session
            self.active_session = session_id
            
        return session
        
    async def update_session(self, updates: Dict[str, Any]):
        """Update active session"""
        if not self.active_session or self.active_session not in self.sessions:
            return
            
        session = self.sessions[self.active_session]
        
        # Update fields
        for key, value in updates.items():
            if hasattr(session, key):
                setattr(session, key, value)
                
        session.last_active = datetime.now()
        
        # Queue for persistence
        await self.state.queue_session_update(session)
        
    async def record_action(self, action: str, data: Dict[str, Any], 
                          reversible: bool = False,
                          undo_data: Optional[Dict] = None):
        """Record an action in history"""
        entry = HistoryEntry(
            id=hashlib.sha256(
                f"{action}{datetime.now().isoformat()}".encode()
            ).hexdigest()[:16],
            action=action,
            timestamp=datetime.now(),
            data=data,
            session_id=self.active_session,
            reversible=reversible,
            undo_data=undo_data
        )
        
        await self.history.add(entry)
        
    async def get_history(self, 
                         action: Optional[str] = None,
                         limit: int = 100,
                         since: Optional[datetime] = None) -> List[HistoryEntry]:
        """Get history entries"""
        return await self.history.get(action, limit, since)
        
    async def undo(self) -> bool:
        """Undo last reversible action"""
        return await self.history.undo()
        
    async def redo(self) -> bool:
        """Redo previously undone action"""
        return await self.history.redo()
        
    async def learn(self, context: str, pattern: Any, weight: float = 1.0):
        """Learn a pattern for semantic memory"""
        await self.semantic.learn(context, pattern, weight)
        
    async def suggest(self, context: str, limit: int = 5) -> List[Any]:
        """Get suggestions from semantic memory"""
        return await self.semantic.suggest(context, limit)
        
    async def save_all(self):
        """Save all memory to persistent storage"""
        await self.cache.flush()
        await self.history.flush()
        await self.state.flush()
        await self.index.flush()
        await self.semantic.flush()
        
    async def clear_all(self):
        """Clear all memory"""
        await self.cache.clear()
        await self.history.clear()
        await self.state.clear()
        await self.index.clear()
        await self.semantic.clear()
        
    async def get_stats(self) -> Dict[str, Any]:
        """Get memory statistics"""
        return {
            'total_memory': self.max_memory,
            'used_memory': await self.memory_monitor.get_usage(),
            'cache_stats': await self.cache.get_stats(),
            'history_stats': await self.history.get_stats(),
            'state_stats': await self.state.get_stats(),
            'index_stats': await self.index.get_stats(),
            'semantic_stats': await self.semantic.get_stats(),
            'sessions': len(self.sessions),
            'active_session': self.active_session
        }
        
    # Helper methods
    
    async def _cleanup_loop(self):
        """Periodic cleanup of expired entries"""
        while True:
            try:
                await asyncio.sleep(60)  # Run every minute
                await self.cache.cleanup_expired()
                await self.history.cleanup_old()
                
                # Clean inactive sessions
                cutoff = datetime.now() - timedelta(hours=24)
                for session_id, session in list(self.sessions.items()):
                    if session.last_active < cutoff and session_id != self.active_session:
                        del self.sessions[session_id]
                        
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"Cleanup error: {e}")
                
    async def _persist_loop(self):
        """Periodic persistence of memory"""
        while True:
            try:
                await asyncio.sleep(300)  # Save every 5 minutes
                await self.save_all()
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"Persist error: {e}")
                
    async def _free_memory(self, required_size: int):
        """Free memory to make space"""
        # Use LRU eviction
        await self.cache.evict_lru(required_size)
        
    def _estimate_size(self, obj: Any) -> int:
        """Estimate size of object in bytes"""
        try:
            if isinstance(obj, (str, bytes)):
                return len(obj)
            elif isinstance(obj, (list, tuple)):
                return sum(self._estimate_size(item) for item in obj)
            elif isinstance(obj, dict):
                return sum(
                    self._estimate_size(k) + self._estimate_size(v)
                    for k, v in obj.items()
                )
            else:
                # Rough estimate using pickle
                return len(pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL))
        except:
            return 1024  # Default estimate


class CacheManager:
    """Manages caching with multiple levels"""
    
    def __init__(self, cache_dir: Path):
        self.cache_dir = cache_dir
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Memory cache (LRU)
        self.memory_cache: OrderedDict[str, MemoryEntry] = OrderedDict()
        self.max_memory_items = 10000
        
        # Disk cache
        self.disk_cache_dir = self.cache_dir / 'disk'
        self.disk_cache_dir.mkdir(exist_ok=True)
        
        # Persistent cache (SQLite)
        self.db_path = self.cache_dir / 'persistent.db'
        self.db_conn = None
        
        # Cache statistics
        self.hits = 0
        self.misses = 0
        
    async def start(self):
        """Initialize cache"""
        # Setup persistent cache
        self.db_conn = sqlite3.connect(str(self.db_path))
        self.db_conn.execute('''
            CREATE TABLE IF NOT EXISTS cache (
                key TEXT PRIMARY KEY,
                value BLOB,
                timestamp REAL,
                ttl INTEGER,
                metadata TEXT
            )
        ''')
        self.db_conn.commit()
        
    async def stop(self):
        """Close cache connections"""
        if self.db_conn:
            self.db_conn.close()
            
    async def set(self, key: str, value: Any, 
                  ttl: Optional[int] = None,
                  level: CacheLevel = CacheLevel.MEMORY,
                  metadata: Optional[Dict] = None) -> bool:
        """Set a cache entry"""
        entry = MemoryEntry(
            key=key,
            value=value,
            timestamp=datetime.now(),
            ttl=ttl,
            size=self._estimate_size(value),
            metadata=metadata or {}
        )
        
        if level == CacheLevel.MEMORY:
            # Add to memory cache
            self.memory_cache[key] = entry
            self.memory_cache.move_to_end(key)
            
            # Evict if necessary
            while len(self.memory_cache) > self.max_memory_items:
                self.memory_cache.popitem(last=False)
                
        elif level == CacheLevel.DISK:
            # Write to disk
            file_path = self.disk_cache_dir / f"{key}.cache"
            compressed = lz4.frame.compress(pickle.dumps(entry))
            
            async with aiofiles.open(file_path, 'wb') as f:
                await f.write(compressed)
                
        elif level == CacheLevel.PERSISTENT:
            # Store in database
            self.db_conn.execute(
                'INSERT OR REPLACE INTO cache VALUES (?, ?, ?, ?, ?)',
                (key, pickle.dumps(value), entry.timestamp.timestamp(),
                 ttl, json.dumps(metadata or {}))
            )
            self.db_conn.commit()
            
        return True
        
    async def get(self, key: str) -> Optional[Any]:
        """Get a cache entry"""
        # Check memory cache first
        if key in self.memory_cache:
            entry = self.memory_cache[key]
            
            # Check TTL
            if entry.ttl:
                age = (datetime.now() - entry.timestamp).total_seconds()
                if age > entry.ttl:
                    del self.memory_cache[key]
                    self.misses += 1
                    return None
                    
            # Update access info
            entry.access_count += 1
            entry.last_accessed = datetime.now()
            self.memory_cache.move_to_end(key)
            
            self.hits += 1
            return entry.value
            
        # Check disk cache
        file_path = self.disk_cache_dir / f"{key}.cache"
        if file_path.exists():
            try:
                async with aiofiles.open(file_path, 'rb') as f:
                    compressed = await f.read()
                    entry = pickle.loads(lz4.frame.decompress(compressed))
                    
                # Check TTL
                if entry.ttl:
                    age = (datetime.now() - entry.timestamp).total_seconds()
                    if age > entry.ttl:
                        file_path.unlink()
                        self.misses += 1
                        return None
                        
                self.hits += 1
                
                # Promote to memory cache
                self.memory_cache[key] = entry
                self.memory_cache.move_to_end(key)
                
                return entry.value
            except:
                pass
                
        # Check persistent cache
        cursor = self.db_conn.execute(
            'SELECT value, timestamp, ttl FROM cache WHERE key = ?',
            (key,)
        )
        row = cursor.fetchone()
        
        if row:
            value, timestamp, ttl = row
            
            # Check TTL
            if ttl:
                age = datetime.now().timestamp() - timestamp
                if age > ttl:
                    self.db_conn.execute('DELETE FROM cache WHERE key = ?', (key,))
                    self.db_conn.commit()
                    self.misses += 1
                    return None
                    
            self.hits += 1
            return pickle.loads(value)
            
        self.misses += 1
        return None
        
    async def delete(self, key: str) -> bool:
        """Delete a cache entry"""
        deleted = False
        
        # Remove from memory
        if key in self.memory_cache:
            del self.memory_cache[key]
            deleted = True
            
        # Remove from disk
        file_path = self.disk_cache_dir / f"{key}.cache"
        if file_path.exists():
            file_path.unlink()
            deleted = True
            
        # Remove from database
        self.db_conn.execute('DELETE FROM cache WHERE key = ?', (key,))
        if self.db_conn.total_changes > 0:
            deleted = True
        self.db_conn.commit()
        
        return deleted
        
    async def search(self, pattern: str, limit: int = 10) -> List[Tuple[str, Any]]:
        """Search cache by pattern"""
        results = []
        
        # Search memory cache
        for key, entry in self.memory_cache.items():
            if pattern.lower() in key.lower():
                results.append((key, entry.value))
                if len(results) >= limit:
                    break
                    
        return results
        
    async def cleanup_expired(self):
        """Remove expired entries"""
        # Memory cache
        expired_keys = []
        for key, entry in self.memory_cache.items():
            if entry.ttl:
                age = (datetime.now() - entry.timestamp).total_seconds()
                if age > entry.ttl:
                    expired_keys.append(key)
                    
        for key in expired_keys:
            del self.memory_cache[key]
            
        # Disk cache
        for file_path in self.disk_cache_dir.iterdir():
            if file_path.suffix == '.cache':
                try:
                    async with aiofiles.open(file_path, 'rb') as f:
                        compressed = await f.read()
                        entry = pickle.loads(lz4.frame.decompress(compressed))
                        
                    if entry.ttl:
                        age = (datetime.now() - entry.timestamp).total_seconds()
                        if age > entry.ttl:
                            file_path.unlink()
                except:
                    # Remove corrupted files
                    file_path.unlink()
                    
        # Database
        cutoff = datetime.now().timestamp()
        self.db_conn.execute(
            'DELETE FROM cache WHERE ttl IS NOT NULL AND timestamp + ttl < ?',
            (cutoff,)
        )
        self.db_conn.commit()
        
    async def evict_lru(self, required_size: int):
        """Evict least recently used items"""
        freed = 0
        
        while freed < required_size and self.memory_cache:
            key, entry = self.memory_cache.popitem(last=False)
            freed += entry.size
            
    async def clear(self):
        """Clear all cache"""
        self.memory_cache.clear()
        
        # Clear disk cache
        for file_path in self.disk_cache_dir.iterdir():
            if file_path.suffix == '.cache':
                file_path.unlink()
                
        # Clear database
        self.db_conn.execute('DELETE FROM cache')
        self.db_conn.commit()
        
    async def flush(self):
        """Flush memory cache to disk"""
        for key, entry in self.memory_cache.items():
            if entry.size > 10240:  # Items larger than 10KB
                await self.set(key, entry.value, entry.ttl, CacheLevel.DISK, entry.metadata)
                
    async def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        hit_rate = self.hits / (self.hits + self.misses) if (self.hits + self.misses) > 0 else 0
        
        # Count disk cache files
        disk_files = len(list(self.disk_cache_dir.glob('*.cache')))
        
        # Count database entries
        cursor = self.db_conn.execute('SELECT COUNT(*) FROM cache')
        db_entries = cursor.fetchone()[0]
        
        return {
            'memory_entries': len(self.memory_cache),
            'disk_entries': disk_files,
            'persistent_entries': db_entries,
            'hits': self.hits,
            'misses': self.misses,
            'hit_rate': hit_rate,
            'memory_size': sum(e.size for e in self.memory_cache.values())
        }
        
    def _estimate_size(self, obj: Any) -> int:
        """Estimate object size"""
        try:
            return len(pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL))
        except:
            return 1024


class HistoryManager:
    """Manages action history and undo/redo"""
    
    def __init__(self, history_dir: Path):
        self.history_dir = history_dir
        self.history_dir.mkdir(parents=True, exist_ok=True)
        
        # In-memory history
        self.history: deque[HistoryEntry] = deque(maxlen=1000)
        self.undo_stack: deque[HistoryEntry] = deque(maxlen=100)
        self.redo_stack: deque[HistoryEntry] = deque(maxlen=100)
        
        # Persistent storage
        self.db_path = self.history_dir / 'history.db'
        self.db_conn = None
        
    async def start(self):
        """Initialize history"""
        self.db_conn = sqlite3.connect(str(self.db_path))
        self.db_conn.execute('''
            CREATE TABLE IF NOT EXISTS history (
                id TEXT PRIMARY KEY,
                action TEXT,
                timestamp REAL,
                data TEXT,
                session_id TEXT,
                reversible INTEGER,
                undo_data TEXT
            )
        ''')
        self.db_conn.commit()
        
        # Load recent history
        await self._load_recent()
        
    async def stop(self):
        """Close history connections"""
        if self.db_conn:
            self.db_conn.close()
            
    async def add(self, entry: HistoryEntry):
        """Add entry to history"""
        # Add to memory
        self.history.append(entry)
        
        # Clear redo stack on new action
        self.redo_stack.clear()
        
        # Add to undo stack if reversible
        if entry.reversible:
            self.undo_stack.append(entry)
            
        # Persist to database
        self.db_conn.execute(
            'INSERT INTO history VALUES (?, ?, ?, ?, ?, ?, ?)',
            (entry.id, entry.action, entry.timestamp.timestamp(),
             json.dumps(entry.data), entry.session_id,
             int(entry.reversible), json.dumps(entry.undo_data) if entry.undo_data else None)
        )
        self.db_conn.commit()
        
    async def get(self, 
                  action: Optional[str] = None,
                  limit: int = 100,
                  since: Optional[datetime] = None) -> List[HistoryEntry]:
        """Get history entries"""
        # Build query
        query = 'SELECT * FROM history WHERE 1=1'
        params = []
        
        if action:
            query += ' AND action = ?'
            params.append(action)
            
        if since:
            query += ' AND timestamp > ?'
            params.append(since.timestamp())
            
        query += ' ORDER BY timestamp DESC LIMIT ?'
        params.append(limit)
        
        cursor = self.db_conn.execute(query, params)
        entries = []
        
        for row in cursor:
            entry = HistoryEntry(
                id=row[0],
                action=row[1],
                timestamp=datetime.fromtimestamp(row[2]),
                data=json.loads(row[3]),
                session_id=row[4],
                reversible=bool(row[5]),
                undo_data=json.loads(row[6]) if row[6] else None
            )
            entries.append(entry)
            
        return entries
        
    async def undo(self) -> bool:
        """Undo last reversible action"""
        if not self.undo_stack:
            return False
            
        entry = self.undo_stack.pop()
        self.redo_stack.append(entry)
        
        # Execute undo
        if entry.undo_data:
            # Trigger undo action
            undo_entry = HistoryEntry(
                id=hashlib.sha256(f"undo_{entry.id}".encode()).hexdigest()[:16],
                action=f"undo_{entry.action}",
                timestamp=datetime.now(),
                data=entry.undo_data,
                session_id=entry.session_id,
                reversible=False
            )
            await self.add(undo_entry)
            
        return True
        
    async def redo(self) -> bool:
        """Redo previously undone action"""
        if not self.redo_stack:
            return False
            
        entry = self.redo_stack.pop()
        self.undo_stack.append(entry)
        
        # Re-execute action
        redo_entry = HistoryEntry(
            id=hashlib.sha256(f"redo_{entry.id}".encode()).hexdigest()[:16],
            action=f"redo_{entry.action}",
            timestamp=datetime.now(),
            data=entry.data,
            session_id=entry.session_id,
            reversible=True,
            undo_data=entry.undo_data
        )
        await self.add(redo_entry)
        
        return True
        
    async def cleanup_old(self):
        """Remove old history entries"""
        cutoff = (datetime.now() - timedelta(days=30)).timestamp()
        
        self.db_conn.execute(
            'DELETE FROM history WHERE timestamp < ? AND reversible = 0',
            (cutoff,)
        )
        self.db_conn.commit()
        
    async def clear(self):
        """Clear all history"""
        self.history.clear()
        self.undo_stack.clear()
        self.redo_stack.clear()
        
        self.db_conn.execute('DELETE FROM history')
        self.db_conn.commit()
        
    async def flush(self):
        """Ensure all history is persisted"""
        # Already persisting on add, so just commit
        self.db_conn.commit()
        
    async def get_stats(self) -> Dict[str, Any]:
        """Get history statistics"""
        cursor = self.db_conn.execute('SELECT COUNT(*) FROM history')
        total_entries = cursor.fetchone()[0]
        
        cursor = self.db_conn.execute(
            'SELECT action, COUNT(*) FROM history GROUP BY action ORDER BY COUNT(*) DESC LIMIT 10'
        )
        top_actions = dict(cursor.fetchall())
        
        return {
            'total_entries': total_entries,
            'memory_entries': len(self.history),
            'undo_stack_size': len(self.undo_stack),
            'redo_stack_size': len(self.redo_stack),
            'top_actions': top_actions
        }
        
    async def _load_recent(self):
        """Load recent history into memory"""
        entries = await self.get(limit=1000)
        
        for entry in reversed(entries):
            self.history.append(entry)
            if entry.reversible:
                self.undo_stack.append(entry)


class StateManager:
    """Manages IDE state and sessions"""
    
    def __init__(self, state_dir: Path):
        self.state_dir = state_dir
        self.state_dir.mkdir(parents=True, exist_ok=True)
        
        # Session storage
        self.sessions_dir = self.state_dir / 'sessions'
        self.sessions_dir.mkdir(exist_ok=True)
        
        # Workspace state
        self.workspace_state = {}
        self.state_file = self.state_dir / 'workspace.json'
        
        # Update queue
        self.update_queue = asyncio.Queue()
        self._update_task = None
        
    async def start(self):
        """Initialize state manager"""
        # Load workspace state
        if self.state_file.exists():
            async with aiofiles.open(self.state_file, 'r') as f:
                self.workspace_state = json.loads(await f.read())
                
        # Start update processor
        self._update_task = asyncio.create_task(self._process_updates())
        
    async def stop(self):
        """Stop state manager"""
        # Process remaining updates
        while not self.update_queue.empty():
            await self._process_single_update()
            
        # Cancel update task
        if self._update_task:
            self._update_task.cancel()
            
        # Save final state
        await self.save_workspace_state()
        
    async def save_session(self, session: SessionState):
        """Save session state"""
        session_file = self.sessions_dir / f"{session.session_id}.json"
        
        async with aiofiles.open(session_file, 'w') as f:
            await f.write(json.dumps(asdict(session), default=str))
            
    async def load_session(self, session_id: str) -> Optional[SessionState]:
        """Load session state"""
        session_file = self.sessions_dir / f"{session_id}.json"
        
        if not session_file.exists():
            return None
            
        try:
            async with aiofiles.open(session_file, 'r') as f:
                data = json.loads(await f.read())
                
            # Convert back to SessionState
            data['created_at'] = datetime.fromisoformat(data['created_at'])
            data['last_active'] = datetime.fromisoformat(data['last_active'])
            
            return SessionState(**data)
        except:
            return None
            
    async def delete_session(self, session_id: str):
        """Delete session state"""
        session_file = self.sessions_dir / f"{session_id}.json"
        if session_file.exists():
            session_file.unlink()
            
    async def list_sessions(self) -> List[str]:
        """List all saved sessions"""
        return [
            f.stem for f in self.sessions_dir.glob('*.json')
        ]
        
    async def queue_session_update(self, session: SessionState):
        """Queue session update for batch processing"""
        await self.update_queue.put(('session', session))
        
    async def save_workspace_state(self):
        """Save workspace state"""
        async with aiofiles.open(self.state_file, 'w') as f:
            await f.write(json.dumps(self.workspace_state, indent=2))
            
    async def get_workspace_state(self, key: str, default: Any = None) -> Any:
        """Get workspace state value"""
        return self.workspace_state.get(key, default)
        
    async def set_workspace_state(self, key: str, value: Any):
        """Set workspace state value"""
        self.workspace_state[key] = value
        await self.update_queue.put(('workspace', None))
        
    async def clear(self):
        """Clear all state"""
        # Remove session files
        for session_file in self.sessions_dir.glob('*.json'):
            session_file.unlink()
            
        # Clear workspace state
        self.workspace_state.clear()
        await self.save_workspace_state()
        
    async def flush(self):
        """Flush pending updates"""
        while not self.update_queue.empty():
            await self._process_single_update()
            
    async def get_stats(self) -> Dict[str, Any]:
        """Get state statistics"""
        session_count = len(list(self.sessions_dir.glob('*.json')))
        
        total_size = sum(
            f.stat().st_size for f in self.sessions_dir.glob('*.json')
        )
        
        return {
            'session_count': session_count,
            'workspace_keys': len(self.workspace_state),
            'total_size': total_size,
            'update_queue_size': self.update_queue.qsize()
        }
        
    async def _process_updates(self):
        """Process update queue"""
        while True:
            try:
                await self._process_single_update()
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"State update error: {e}")
                
    async def _process_single_update(self):
        """Process a single update"""
        try:
            update_type, data = await asyncio.wait_for(
                self.update_queue.get(), timeout=1.0
            )
            
            if update_type == 'session':
                await self.save_session(data)
            elif update_type == 'workspace':
                await self.save_workspace_state()
                
        except asyncio.TimeoutError:
            pass


class IndexManager:
    """Manages searchable indexes"""
    
    def __init__(self, index_dir: Path):
        self.index_dir = index_dir
        self.index_dir.mkdir(parents=True, exist_ok=True)
        
        # Different indexes
        self.file_index = {}  # file_path -> metadata
        self.symbol_index = {}  # symbol -> locations
        self.search_index = {}  # term -> documents
        
        # Index files
        self.file_index_path = self.index_dir / 'files.json'
        self.symbol_index_path = self.index_dir / 'symbols.json'
        self.search_index_path = self.index_dir / 'search.json'
        
    async def start(self):
        """Load indexes"""
        for index_path, index_dict in [
            (self.file_index_path, self.file_index),
            (self.symbol_index_path, self.symbol_index),
            (self.search_index_path, self.search_index)
        ]:
            if index_path.exists():
                async with aiofiles.open(index_path, 'r') as f:
                    data = json.loads(await f.read())
                    index_dict.update(data)
                    
    async def stop(self):
        """Save indexes"""
        await self.flush()
        
    async def index_file(self, file_path: str, content: str, metadata: Dict[str, Any]):
        """Index a file"""
        # Update file index
        self.file_index[file_path] = {
            'size': len(content),
            'modified': datetime.now().isoformat(),
            'language': metadata.get('language'),
            'encoding': metadata.get('encoding', 'utf-8'),
            'checksum': hashlib.md5(content.encode()).hexdigest()
        }
        
        # Extract and index symbols
        symbols = self._extract_symbols(content, metadata.get('language'))
        for symbol in symbols:
            if symbol['name'] not in self.symbol_index:
                self.symbol_index[symbol['name']] = []
                
            self.symbol_index[symbol['name']].append({
                'file': file_path,
                'line': symbol['line'],
                'type': symbol['type']
            })
            
        # Update search index
        terms = self._extract_terms(content)
        for term in terms:
            if term not in self.search_index:
                self.search_index[term] = set()
                
            self.search_index[term].add(file_path)
            
    async def remove_file(self, file_path: str):
        """Remove file from indexes"""
        # Remove from file index
        if file_path in self.file_index:
            del self.file_index[file_path]
            
        # Remove from symbol index
        for symbol, locations in list(self.symbol_index.items()):
            self.symbol_index[symbol] = [
                loc for loc in locations if loc['file'] != file_path
            ]
            if not self.symbol_index[symbol]:
                del self.symbol_index[symbol]
                
        # Remove from search index
        for term, files in list(self.search_index.items()):
            if file_path in files:
                files.remove(file_path)
            if not files:
                del self.search_index[term]
                
    async def search_files(self, query: str) -> List[str]:
        """Search files by query"""
        terms = self._extract_terms(query)
        
        if not terms:
            return []
            
        # Find files containing all terms
        result_sets = [
            self.search_index.get(term, set())
            for term in terms
        ]
        
        if not result_sets:
            return []
            
        # Intersection of all sets
        results = result_sets[0]
        for s in result_sets[1:]:
            results = results.intersection(s)
            
        return list(results)
        
    async def find_symbol(self, symbol_name: str) -> List[Dict]:
        """Find symbol locations"""
        return self.symbol_index.get(symbol_name, [])
        
    async def get_file_info(self, file_path: str) -> Optional[Dict]:
        """Get file metadata"""
        return self.file_index.get(file_path)
        
    async def clear(self):
        """Clear all indexes"""
        self.file_index.clear()
        self.symbol_index.clear()
        self.search_index.clear()
        
    async def flush(self):
        """Save indexes to disk"""
        # Convert sets to lists for JSON serialization
        search_index_serializable = {
            term: list(files)
            for term, files in self.search_index.items()
        }
        
        for index_path, index_data in [
            (self.file_index_path, self.file_index),
            (self.symbol_index_path, self.symbol_index),
            (self.search_index_path, search_index_serializable)
        ]:
            async with aiofiles.open(index_path, 'w') as f:
                await f.write(json.dumps(index_data, indent=2))
                
    async def get_stats(self) -> Dict[str, Any]:
        """Get index statistics"""
        return {
            'indexed_files': len(self.file_index),
            'indexed_symbols': len(self.symbol_index),
            'search_terms': len(self.search_index),
            'total_symbol_refs': sum(
                len(locs) for locs in self.symbol_index.values()
            )
        }
        
    def _extract_symbols(self, content: str, language: Optional[str]) -> List[Dict]:
        """Extract symbols from code"""
        symbols = []
        
        if language == 'python':
            # Simple regex-based extraction
            import re
            
            # Functions
            for match in re.finditer(r'^def\s+(\w+)', content, re.MULTILINE):
                symbols.append({
                    'name': match.group(1),
                    'type': 'function',
                    'line': content[:match.start()].count('\n') + 1
                })
                
            # Classes
            for match in re.finditer(r'^class\s+(\w+)', content, re.MULTILINE):
                symbols.append({
                    'name': match.group(1),
                    'type': 'class',
                    'line': content[:match.start()].count('\n') + 1
                })
                
        # Add more language support as needed
        
        return symbols
        
    def _extract_terms(self, content: str) -> Set[str]:
        """Extract search terms from content"""
        import re
        
        # Simple word extraction
        words = re.findall(r'\b\w+\b', content.lower())
        
        # Filter common words
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for'}
        
        return {
            word for word in words
            if len(word) > 2 and word not in stop_words
        }


class SemanticMemory:
    """Semantic memory for pattern learning and suggestions"""
    
    def __init__(self, semantic_dir: Path):
        self.semantic_dir = semantic_dir
        self.semantic_dir.mkdir(parents=True, exist_ok=True)
        
        # Pattern storage
        self.patterns = defaultdict(list)  # context -> patterns
        self.pattern_weights = defaultdict(float)  # pattern -> weight
        
        # Neural embeddings (simplified)
        self.embeddings = {}  # pattern -> vector
        self.embedding_dim = 128
        
        # Persistence
        self.patterns_file = self.semantic_dir / 'patterns.json'
        self.embeddings_file = self.semantic_dir / 'embeddings.npy'
        
    async def start(self):
        """Load semantic memory"""
        if self.patterns_file.exists():
            async with aiofiles.open(self.patterns_file, 'r') as f:
                data = json.loads(await f.read())
                for context, patterns in data['patterns'].items():
                    self.patterns[context] = patterns
                self.pattern_weights.update(data.get('weights', {}))
                
        if self.embeddings_file.exists():
            self.embeddings = np.load(str(self.embeddings_file), allow_pickle=True).item()
            
    async def stop(self):
        """Save semantic memory"""
        await self.flush()
        
    async def learn(self, context: str, pattern: Any, weight: float = 1.0):
        """Learn a pattern in context"""
        pattern_str = str(pattern)
        
        # Add to patterns
        if pattern_str not in self.patterns[context]:
            self.patterns[context].append(pattern_str)
            
        # Update weight
        self.pattern_weights[pattern_str] += weight
        
        # Generate embedding if needed
        if pattern_str not in self.embeddings:
            self.embeddings[pattern_str] = self._generate_embedding(pattern_str)
            
    async def suggest(self, context: str, limit: int = 5) -> List[Any]:
        """Get suggestions for context"""
        # Get patterns for context
        context_patterns = self.patterns.get(context, [])
        
        if not context_patterns:
            # Try similar contexts
            similar_contexts = self._find_similar_contexts(context)
            for sim_context in similar_contexts[:3]:
                context_patterns.extend(self.patterns.get(sim_context, []))
                
        # Score patterns
        scored_patterns = []
        for pattern in context_patterns:
            score = self.pattern_weights.get(pattern, 0)
            
            # Boost by similarity to context
            if pattern in self.embeddings:
                context_embedding = self._generate_embedding(context)
                similarity = self._cosine_similarity(
                    self.embeddings[pattern],
                    context_embedding
                )
                score *= (1 + similarity)
                
            scored_patterns.append((pattern, score))
            
        # Sort by score
        scored_patterns.sort(key=lambda x: x[1], reverse=True)
        
        return [pattern for pattern, _ in scored_patterns[:limit]]
        
    async def search(self, query: str, limit: int = 10) -> List[Tuple[str, Any]]:
        """Search semantic memory"""
        results = []
        query_embedding = self._generate_embedding(query)
        
        # Search all patterns
        for pattern, embedding in self.embeddings.items():
            similarity = self._cosine_similarity(query_embedding, embedding)
            if similarity > 0.5:  # Threshold
                results.append((pattern, similarity))
                
        # Sort by similarity
        results.sort(key=lambda x: x[1], reverse=True)
        
        return results[:limit]
        
    async def forget(self, pattern: str):
        """Remove a pattern"""
        # Remove from all contexts
        for context_patterns in self.patterns.values():
            if pattern in context_patterns:
                context_patterns.remove(pattern)
                
        # Remove weight and embedding
        self.pattern_weights.pop(pattern, None)
        self.embeddings.pop(pattern, None)
        
    async def clear(self):
        """Clear semantic memory"""
        self.patterns.clear()
        self.pattern_weights.clear()
        self.embeddings.clear()
        
    async def flush(self):
        """Save semantic memory"""
        # Save patterns
        data = {
            'patterns': dict(self.patterns),
            'weights': dict(self.pattern_weights)
        }
        
        async with aiofiles.open(self.patterns_file, 'w') as f:
            await f.write(json.dumps(data, indent=2))
            
        # Save embeddings
        if self.embeddings:
            np.save(str(self.embeddings_file), self.embeddings)
            
    async def get_stats(self) -> Dict[str, Any]:
        """Get semantic memory statistics"""
        return {
            'contexts': len(self.patterns),
            'unique_patterns': len(set(
                p for patterns in self.patterns.values() for p in patterns
            )),
            'embeddings': len(self.embeddings),
            'average_weight': (
                sum(self.pattern_weights.values()) / len(self.pattern_weights)
                if self.pattern_weights else 0
            )
        }
        
    def _generate_embedding(self, text: str) -> np.ndarray:
        """Generate embedding for text (simplified)"""
        # Simple hash-based embedding
        hash_value = hashlib.sha256(text.encode()).digest()
        
        # Convert to vector
        embedding = np.frombuffer(hash_value, dtype=np.uint8)[:self.embedding_dim]
        
        # Normalize
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
            
        return embedding
        
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """Calculate cosine similarity"""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 * norm2 == 0:
            return 0
            
        return dot_product / (norm1 * norm2)
        
    def _find_similar_contexts(self, context: str) -> List[str]:
        """Find similar contexts"""
        context_embedding = self._generate_embedding(context)
        similar = []
        
        for other_context in self.patterns.keys():
            if other_context != context:
                other_embedding = self._generate_embedding(other_context)
                similarity = self._cosine_similarity(context_embedding, other_embedding)
                if similarity > 0.7:
                    similar.append((other_context, similarity))
                    
        similar.sort(key=lambda x: x[1], reverse=True)
        return [ctx for ctx, _ in similar]


class MemoryMonitor:
    """Monitor memory usage and pressure"""
    
    def __init__(self, max_memory: int):
        self.max_memory = max_memory
        self.current_usage = 0
        self.pressure_callbacks = []
        
    async def monitor(self):
        """Monitor memory usage"""
        while True:
            try:
                # Get process memory
                process = psutil.Process()
                self.current_usage = process.memory_info().rss
                
                # Check pressure
                if self.current_usage > self.max_memory * 0.9:
                    await self._trigger_pressure_callbacks()
                    
                await asyncio.sleep(5)  # Check every 5 seconds
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"Memory monitoring error: {e}")
                
    async def can_allocate(self, size: int) -> bool:
        """Check if can allocate memory"""
        return self.current_usage + size < self.max_memory
        
    async def get_usage(self) -> int:
        """Get current memory usage"""
        return self.current_usage
        
    def add_pressure_callback(self, callback: Callable):
        """Add memory pressure callback"""
        self.pressure_callbacks.append(callback)
        
    async def _trigger_pressure_callbacks(self):
        """Trigger pressure callbacks"""
        for callback in self.pressure_callbacks:
            try:
                await callback()
            except Exception as e:
                print(f"Pressure callback error: {e}")


# WebSocket handlers

async def handle_memory_request(websocket, message: Dict):
    """Handle memory requests from WebSocket"""
    workspace_root = message.get('workspace_root', '.')
    manager = MemoryManager(workspace_root)
    
    action = message.get('action')
    
    if action == 'remember':
        key = message.get('key')
        value = message.get('value')
        ttl = message.get('ttl')
        metadata = message.get('metadata')
        
        success = await manager.remember(key, value, ttl, metadata)
        await websocket.send(json.dumps({
            'type': 'memory_result',
            'action': 'remember',
            'success': success
        }))
        
    elif action == 'recall':
        key = message.get('key')
        value = await manager.recall(key)
        
        await websocket.send(json.dumps({
            'type': 'memory_result',
            'action': 'recall',
            'value': value
        }))
        
    elif action == 'search':
        pattern = message.get('pattern')
        limit = message.get('limit', 10)
        results = await manager.search(pattern, limit)
        
        await websocket.send(json.dumps({
            'type': 'memory_result',
            'action': 'search',
            'results': results
        }))
        
    elif action == 'record_action':
        action_name = message.get('action_name')
        data = message.get('data')
        reversible = message.get('reversible', False)
        undo_data = message.get('undo_data')
        
        await manager.record_action(action_name, data, reversible, undo_data)
        
    elif action == 'undo':
        success = await manager.undo()
        await websocket.send(json.dumps({
            'type': 'memory_result',
            'action': 'undo',
            'success': success
        }))
        
    elif action == 'get_stats':
        stats = await manager.get_stats()
        await websocket.send(json.dumps({
            'type': 'memory_result',
            'action': 'stats',
            'stats': stats
        }))